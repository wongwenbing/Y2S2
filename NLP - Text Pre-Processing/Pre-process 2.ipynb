{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Pre-Processing Part II**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "- remove punctuation and change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='''\n",
    "The CPF LIFE estimator allows us to do two things: estimate our CPF LIFE monthly payouts and bequests, on\n",
    "the different CPF LIFE retirement sums, CPF LIFE plans and starting age estimate how much we need to our \n",
    "Retirement Account (RA) in order to receive our desired CPF LIFE monthly payouts.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cpf', 'life', 'estimator', 'allows', 'us', 'to', 'do', 'two', 'things', 'estimate', 'our', 'cpf', 'life', 'monthly', 'payouts', 'and', 'bequests', 'on', 'the', 'different', 'cpf', 'life', 'retirement', 'sums', 'cpf', 'life', 'plans', 'and', 'starting', 'age', 'estimate', 'how', 'much', 'we', 'need', 'to', 'our', 'retirement', 'account', 'ra', 'in', 'order', 'to', 'receive', 'our', 'desired', 'cpf', 'life', 'monthly', 'payouts']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(txt)\n",
    "words=[]\n",
    "for t in tokens:\n",
    "    if not(t in string.punctuation):\n",
    "        words.append(t.lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "- remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/wong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "tokens=word_tokenize(txt)\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=list(stopwords.words('english'))\n",
    "\n",
    "final_tokens=[]\n",
    "for t in tokens:\n",
    "    if (not (t in string.punctuation) and not(t.lower() in stopword)):\n",
    "        final_tokens.append(t)\n",
    "        \n",
    "print(final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPF', 'LIFE', 'estimator', 'allows', 'us', 'two', 'things', 'estimate', 'CPF', 'LIFE', 'monthly', 'payouts', 'bequests', 'different', 'CPF', 'LIFE', 'retirement', 'sums', 'CPF', 'LIFE', 'plans', 'starting', 'age']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/wong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "txt='''The CPF LIFE estimator allows us to do two things estimate our CPF LIFE monthly \n",
    "payouts and bequests on the different CPF LIFE retirement sums CPF LIFE plans and starting age'''\n",
    "tokens=word_tokenize(txt) #tokenise the words first\n",
    "nltk.download('stopwords') #download stopwords\n",
    "stopwords=list(stopwords.words('english')) #obtain stopwords as a list\n",
    "words=[]\n",
    "for word in tokens: \n",
    "    if word.lower() not in stopwords: #if words are not in stopwords list --> relevant\n",
    "        words.append(word)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "- remove punctuation\n",
    "- add new stopwords - often, daily, would\n",
    "- remove stopwords - not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='''Some interviewees told TODAY that there are often daily reminders of the ones they have lost \n",
    " to the virus, and that it is hard not to question whether the outcome would have been different \n",
    " had they taken different decisions and acted differently.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/wong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_updated=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_updated.add('often')\n",
    "# stopwords_updated.add('daily')\n",
    "# stopwords_updated.add('would')\n",
    "stopwords_updated.remove('not')\n",
    "\n",
    "update=('often','daily','would')\n",
    "stopwords_updated.update(update)\n",
    "\n",
    "# stopwords_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interviewees', 'told', 'TODAY', 'reminders', 'ones', 'lost', 'virus', 'hard', 'not', 'question', 'whether', 'outcome', 'different', 'taken', 'different', 'decisions', 'acted', 'differently']\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for text in tokens: \n",
    "    if text not in string.punctuation and text.lower() not in stopwords_updated: \n",
    "        words.append(text)\n",
    "        \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "- Lemmatise all nouns + display nouns that make the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''\n",
    " Schools are poised to resume overseas trips for students\n",
    " , with Covid-19 travel restrictions eased\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatiser=WordNetLemmatizer()\n",
    "tokens=word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schools  ==>  school\n",
      "trips  ==>  trip\n",
      "students  ==>  student\n",
      "Covid-19  ==>  covid-19\n",
      "restrictions  ==>  restriction\n"
     ]
    }
   ],
   "source": [
    "token_words_lem=[]\n",
    "for word in tokens:\n",
    "    token_words_lem.append(lemmatiser.lemmatize(word.lower(),pos='n'))\n",
    "    #Schools will not be lemmatised as the string is upper case\n",
    "    # as such, need to .lower() to match\n",
    "for i in range(len(tokens)):\n",
    "    # print(tokens[i])\n",
    "    if tokens[i] != token_words_lem[i]:\n",
    "        print(tokens[i], \" ==> \", token_words_lem[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "- List all the POS tag for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Schools', 'NNS'), ('are', 'VBP'), ('poised', 'VBN'), ('to', 'TO'), ('resume', 'VB'), ('overseas', 'JJ'), ('trips', 'NNS'), ('for', 'IN'), ('students', 'NNS'), (',', ','), ('with', 'IN'), ('Covid-19', 'NNP'), ('travel', 'NN'), ('restrictions', 'NNS'), ('eased', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "txt = '''\n",
    " Schools are poised to resume overseas trips for students\n",
    " , with Covid-19 travel restrictions eased\n",
    "'''\n",
    "post = pos_tag(word_tokenize(txt))\n",
    "print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 \n",
    "- use compound words to chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "txt='''\n",
    " Mr Heng Swee Keat will deliver a Ministerial Statement on\n",
    " additional support measures for COVID-19 pandemic.\n",
    "'''\n",
    "compound=[(\"ministerial\", \"statement\"), ('heng','swee','keat')]\n",
    "mwe_tokeniser=MWETokenizer(compound,separator=' ')\n",
    "tokens_compounded=mwe_tokeniser.tokenize(word_tokenize(txt.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'heng swee keat', 'will', 'deliver', 'a', 'ministerial statement', 'on', 'additional', 'support', 'measures', 'for', 'covid-19', 'pandemic', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_compounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

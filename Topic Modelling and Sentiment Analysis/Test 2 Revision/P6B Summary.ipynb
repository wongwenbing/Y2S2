{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical 6B: Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries to import\n",
    "```Python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Classification**   \n",
    "- classifier is training with **labelled training data**\n",
    "\n",
    "*Create list of movie review document*\n",
    "```Python\n",
    "# Load the movie review dataset\n",
    "from nltk.corpus import movie_reviews \n",
    "reviews = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    sentiment, filename = fileid.split('/')\n",
    "    reviews.append((filename, movie_reviews.raw(fileid), sentiment))\n",
    "df = pd.DataFrame(reviews, columns=['filename', 'text', 'sentiment'])\n",
    "print(df.shape)\n",
    "display(df.head())\n",
    "# Plotting the Sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(df['sentiment']).plot.bar(title=\"Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**\n",
    "- use top-N words feature\n",
    "\n",
    "*Fetching words from corpus*\n",
    "```Python\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "all_words = [word.lower() for sent in df.text for word in word_tokenize(sent)\n",
    "# print first 10 words\n",
    "print (all_words[:10])\n",
    "```\n",
    "\n",
    "*Create frequency distribution of words*: calculate occurences of each word in entire list of words\n",
    "```Python\n",
    "from nltk import FreqDist\n",
    "all_words_frequency = FreqDist(all_words)\n",
    "print (all_words_frequency)\n",
    "# print 10 most frequently occurring words\n",
    "print (all_words_frequency.most_common(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remove Punctuation and Stopwords with GenSim* \n",
    "```Python\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stopwords_english = stopwords.words('english')\n",
    "def clean(doc):\n",
    "    all_words_clean = []\n",
    "    for word in doc:\n",
    "        if word not in stopwords_english and not word.isdigit():\n",
    "            punc_free = ''.join([ch for ch in word if ch not in string.punctuation])\n",
    "            if len(punc_free)>2 and not word.isdigit():\n",
    "                all_words_clean.append(porter_stemmer.stem(punc_free))\n",
    "    return all_words_clean\n",
    "all_words_clean = clean(all_words)\n",
    "# print the first 10 words\n",
    "print (all_words_clean[:10])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding frequency distribution of new list after removing stopwords**\n",
    "```Python\n",
    "all_words_frequency = FreqDist(all_words_clean)\n",
    "print (all_words_frequency)\n",
    "\n",
    "# print 10 most frequently occurring words\n",
    "print (all_words_frequency.most_common(10))\n",
    "```\n",
    "Previously, before removing stopwords and punctuation, the frequency distribution was:\n",
    "\n",
    "> **FreqDist with 46462 samples and 1525039 outcomes**\n",
    "\n",
    "Now, the frequency distribution is:\n",
    "\n",
    "> **FreqDist with 30899 samples and 686219 outcomes**\n",
    "\n",
    "This shows that after removing around 10000 stop words, numbers and punctuation, the outcomes/words number has reduced to around half of the original size.\n",
    "\n",
    "The **most common words** or highly occurring words list has also got meaningful words in the list. Before, the first 10 frequently occurring words were only stop-words and punctuations.\n",
    "\n",
    "### Create Word Feature using 2000 most frequently occurring words\n",
    "\n",
    "We take 2000 most frequently occurring words as our feature.\n",
    "```Python\n",
    "print (len(all_words_frequency)) \n",
    " \n",
    "# get 2000 frequently occuring words\n",
    "most_common_words = all_words_frequency.most_common(2000)\n",
    "\n",
    "# print the first 10 most frequently occuring words\n",
    "print (most_common_words[:10])\n",
    "\n",
    "# print the last 10 most frequently occuring words\n",
    "print (most_common_words[1990:])\n",
    "\n",
    "# the most common words list's elements are in the form of tuple get \n",
    "# only the first element of each tuple of the word list\n",
    "word_features = [item[0] for item in most_common_words]\n",
    "print (word_features[:10])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Feature Set**\n",
    "- apply text preprocessing through loops for the reviews\n",
    "```Python\n",
    "df['text'] = df['text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df['text'] = df['text'].apply(lambda x: clean(x))  \n",
    "df.head()\n",
    "```\n",
    "- create feature set to train classifier: checks if words in given document are present in word_features_list or not\n",
    "```Python\n",
    "def document_features(df, stemmed_tokens):\n",
    "    doc_features = []\n",
    "    for index, row in df.iterrows():\n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            # get term occurence: true if it's in the word_features, false if it's not\n",
    "            features[word] = (word in row[stemmed_tokens])\n",
    "        doc_features.append(features)\n",
    "    return doc_features\n",
    "\n",
    "feature_set = pd.DataFrame(document_features(df, 'text'), index = df.index)\n",
    "feature_set.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Classifier**\n",
    "- create separate train and test set. Use first 400 elemnts of feature as test and the rest as train. generally use 80/20\n",
    "```Python\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_set\n",
    "y = df[df.columns[-1:]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print (y_train.sentiment.value_counts(normalize=True))\n",
    "\n",
    "#plot chart\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=y_train, x='sentiment')\n",
    "```\n",
    "*Use Decision Tree Classifier to train*\n",
    "```Python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, classifier.predict(X_test)))\n",
    "\n",
    "# accuracy score\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy Score: \" + str(accuracy_score(y_test, y_pred)))\n",
    "```\n",
    "Print Confusion Matrix\n",
    "```Python\n",
    "# Function to create a confusion matrix \n",
    "def conf_matrix(y_test, pred_test):    \n",
    "    \n",
    "    # Creating a confusion matrix\n",
    "    con_mat = confusion_matrix(y_test, pred_test)\n",
    "    con_mat = pd.DataFrame(con_mat, range(2), range(2))\n",
    "   \n",
    "    #Ploting the confusion matrix\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.set(font_scale=1.5) \n",
    "    sns.heatmap(con_mat, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap='Blues', cbar=False)\n",
    "    \n",
    "#Ploting the confusion matrix\n",
    "conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words using TF-IDF feature set**\n",
    "- create dictionary of unique words and calculate term weights for text feature.\n",
    "```Python\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df['text'])\n",
    "vocab_len = len(mydict)\n",
    "\n",
    "def get_bow_features(df, stemmed_tokens):\n",
    "    test_features = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Converting the tokens into the format that the model requires\n",
    "        features = gensim.matutils.corpus2csc([mydict.doc2bow(row[stemmed_tokens])],num_terms=vocab_len).toarray()[:,0]\n",
    "        test_features.append(features)\n",
    "    return test_features\n",
    "\n",
    "header = \",\".join(str(mydict[ele]) for ele in range(vocab_len))\n",
    "\n",
    "bow_features = pd.DataFrame(get_bow_features(df, 'text'),                            \n",
    "                            columns=header.split(','), index = df.index)\n",
    "bow_features.head()\n",
    "\n",
    "#CREATE TERM WEIGHTS WITH TF-IDF\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df['text'])\n",
    "vocab_len = len(mydict)\n",
    "corpus = [mydict.doc2bow(line) for line in df['text']]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "def get_tfidf_features(df, stemmed_tokens):\n",
    "    test_features_tfidf = []\n",
    "    for index, row in df.iterrows():\n",
    "        doc = mydict.doc2bow(row[stemmed_tokens])\n",
    "        # Converting the tokens into the formet that the model requires\n",
    "        features = gensim.matutils.corpus2csc([tfidf_model[doc]], num_terms=vocab_len).toarray()[:,0]\n",
    "        test_features_tfidf.append(features)\n",
    "    return test_features_tfidf\n",
    "\n",
    "header = \",\".join(str(mydict[ele]) for ele in range(vocab_len))\n",
    "\n",
    "tfidf_features = pd.DataFrame(get_tfidf_features(df, 'text'),                            \n",
    "                            columns=header.split(','), index = df.index)\n",
    "tfidf_features.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Classifier + Accuracy calculation**\n",
    "```Python\n",
    "X = tfidf_features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#using decision tree\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "# classification report\n",
    "print(classification_report(y_test, classifier.predict(X_test)))\n",
    "# accuracy score\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy Score: \" + str(accuracy_score(y_test, y_pred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

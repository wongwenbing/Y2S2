{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d22e4c-4059-4326-b949-288384a11ba7",
   "metadata": {},
   "source": [
    "Practical 7B: Topic Modelling \n",
    "\n",
    "**Import library + download pacxkages**\n",
    "```Python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b3042-cf6f-4442-8fee-59c3a19ec83d",
   "metadata": {},
   "source": [
    "**Reading in source files from directory** \n",
    "- given a zip file of 250 files of various categories,  unzip the file and read the file directly, append to the corpus to form a full corpus of words\n",
    "```Python\n",
    "#r is the raw string literals so that windows path slash won't create problem \n",
    "data_folder = Path(r'news')\n",
    "\n",
    "#read each file from the directory into an array and name it corpus\n",
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for filename in data_folder.iterdir():\n",
    "   fp = open(str(filename), 'r', encoding='latin1')\n",
    "   corpus.append(fp.read())\n",
    "   #keep the filename for later use\n",
    "   filenames.append(filename.name)\n",
    "   fp.close()\n",
    "\n",
    "print(corpus.__len__())\n",
    "corpus\n",
    "```\n",
    "\n",
    "**Pre-Processing**: stopwords removal, puncutation removal and lemmatization using WordNetLemmatizer()\n",
    "```Python\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "#clean up conntent and keep in the doc_clean variable\n",
    "def clean(doc):\n",
    "    punc_free = ''.join([ch for ch in doc.lower() if ch not in exclude]) #remove punctuations\n",
    "    stop_free = ' '.join([i for i in punc_free.split() if i not in stop]) #remove stopwords\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in stop_free.split()) #lemmatisation\n",
    "    #stemmed = ' '.join(stemmer.stem(word) for word in normalized.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in corpus]\n",
    "```\n",
    "\n",
    "**Preparing word representation**: use gensim library to do term frequency word representation\n",
    "```Python\n",
    "dictionary = corpora.Dictionary(doc_clean) #use gensium corpora to create data structure keeping all unique words\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] #use dictionary to create doc-term matrix for each of doc / file using bag of words approach\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3bad7-a05d-498d-8db5-6d9ad6f5da87",
   "metadata": {},
   "source": [
    "**Creating LDA model**: uses gensim lda models to set value of 5 for the first model to specify the number of topics for LDA\n",
    "```Python\n",
    "topic_num = 5\n",
    "word_num = 5\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = topic_num, id2word = dictionary, passes=20)\n",
    "\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))\n",
    "# Compute Perplexity\n",
    "print('Perplexity: ', ldamodel.log_perplexity(doc_term_matrix))\n",
    "```\n",
    "Some pointers: \n",
    "- the results and topics are often difficult to identify a category and may not be meaningful\n",
    "- how do you determine suitable number to use?\n",
    "  - use perplexity value (statistical measure of how well probability model predicts sample. benefit comes when comparing different LDA models and model with lower perplexity value is considered better\n",
    "- **increasing topic_num** to a large # **MAY NOT HELP** in understanding categories (unless prior knoweledge of possible large value), thus sacrificing clarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7173b5-8245-4353-8ea8-5841c1d04672",
   "metadata": {},
   "source": [
    "*After Modelling Observations*  \n",
    "- some words are appearing in list of topics (like said, mr) --> can be considered as stopwords and generic stopwords cannot handle domain specific words\n",
    "```Python\n",
    "#addon to stop words\n",
    "domain_stop = [\"said\", \"mr\"]\n",
    "stop.update(domain_stop)\n",
    "#add stemming to pre-processing step. Stemming is done after lemmatization\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = ' '.join(stemmer.stem(word) for word in normalized.split())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ead07-064e-45e4-ac72-b227769c0659",
   "metadata": {},
   "source": [
    "**Retrieving topic details** \n",
    "- Part 1: find out file name and corresponding topic ids with probability. Given that LDA is probability in modelling mixture of topics on given content, LDA assign topic ids with probability to indicate content can potentially has more than topic\n",
    "```Python\n",
    "print('\\nFile name and its corresponding topic id with probability:')\n",
    "dic_topic_doc = {}\n",
    "for index, doc in enumerate(doc_clean):\n",
    "    #for doc in doc_clean:\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    \n",
    "    #get topic distribution of the ldamodel\n",
    "    t = ldamodel.get_document_topics(bow)\n",
    "    \n",
    "    #sort the probability value in descending order to extract the top contributing topic id\n",
    "    sorted_t = sorted(t, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #print only the filename \n",
    "    print(filenames[index],sorted_t)\n",
    "    \n",
    "    #get the top scoring item\n",
    "    top_item = sorted_t.pop(0)\n",
    "    \n",
    "    #create dictionary and keep key as topic id and filename and probability in tuple as value\n",
    "    dic_topic_doc.setdefault(top_item[0],[]).append((filenames[index],top_item[1]))\n",
    "```\n",
    "\n",
    "Part 2: Making use of the above information, and transform to extract list of topic id, number of files (belong to topic) and list of file names with probability \n",
    "```Python\n",
    "#print out identified topic id and associated\n",
    "print('\\nTopic id, number of documents, list of documents with probability and represented topic words:')\n",
    "\n",
    "for key,value in dic_topic_doc.items():\n",
    "    sorted_value = sorted(value, key=lambda x: x[1], reverse=True)\n",
    "    print(key,len(value),sorted_value)\n",
    "    #print the topic word and most represented doc\n",
    "    print(ldamodel.print_topic(key,word_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d649f-7abe-455c-9b13-979fc6245765",
   "metadata": {},
   "source": [
    "The interpretation of the result, based on the below output:\n",
    "\n",
    "0 13 [('206.txt', 0.99757373), ('112.txt', 0.99581325), ('221 .txt', 0.99573374) … <br />\n",
    "0.005*\"said\" + 0.005*\"network\" + 0.005*\"business\" + 0.004*\"uk\" + 0.004*\"could\"<br />\n",
    "1 28 [('111.txt', 0.9982385), ('245.txt', 0.9976861), ('127.txt', 0.9975066),….<br />\n",
    "0.007*\"people\" + 0.007*\"would\" + 0.006*\"said\" + 0.005*\"blair\" + 0.005*\"party\"\n",
    "\n",
    "means that topic id 0 has 13 files identified and 206.txt is assigned with the highest probability, followed by 112.txt and so on. Python starts its index with 0 but essentially, topic id 0 is the first topic identified.\n",
    "\n",
    "Similarly, the next is topic id 1 with 28 files identified and 111.txt is assigned with the highest probability, followed by 245.txt and so on.\n",
    "\n",
    "\n",
    "**Visualize Topics and Keywords**\n",
    "\n",
    "Now, we are ready to visualize our LDA model.\n",
    "\n",
    "The following code uses the pyLDAvis tool to visualize the fit of your LDA model across topics and their top words.\n",
    "> pip install pyLDAvis\n",
    "```Python\n",
    "# plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# visualize the topics and keywords\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, dictionary)\n",
    "vis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69b391-9902-4234-9772-182b6f93ce19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

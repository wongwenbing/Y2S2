{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d22e4c-4059-4326-b949-288384a11ba7",
   "metadata": {},
   "source": [
    "Practical 7B: Topic Modelling \n",
    "\n",
    "**Import library + download pacxkages**\n",
    "```Python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b3042-cf6f-4442-8fee-59c3a19ec83d",
   "metadata": {},
   "source": [
    "**Reading in source files from directory** \n",
    "- given a zip file of 250 files of various categories,  unzip the file and read the file directly, append to the corpus to form a full corpus of words\n",
    "```Python\n",
    "#r is the raw string literals so that windows path slash won't create problem \n",
    "data_folder = Path(r'news')\n",
    "\n",
    "#read each file from the directory into an array and name it corpus\n",
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for filename in data_folder.iterdir():\n",
    "   fp = open(str(filename), 'r', encoding='latin1')\n",
    "   corpus.append(fp.read())\n",
    "   #keep the filename for later use\n",
    "   filenames.append(filename.name)\n",
    "   fp.close()\n",
    "\n",
    "print(corpus.__len__())\n",
    "corpus\n",
    "```\n",
    "\n",
    "**Pre-Processing**: stopwords removal, puncutation removal and lemmatization using WordNetLemmatizer()\n",
    "```Python\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "#clean up conntent and keep in the doc_clean variable\n",
    "def clean(doc):\n",
    "    punc_free = ''.join([ch for ch in doc.lower() if ch not in exclude]) #remove punctuations\n",
    "    stop_free = ' '.join([i for i in punc_free.split() if i not in stop]) #remove stopwords\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in stop_free.split()) #lemmatisation\n",
    "    #stemmed = ' '.join(stemmer.stem(word) for word in normalized.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in corpus]\n",
    "```\n",
    "\n",
    "**Preparing word representation**: use gensim library to do term frequency word representation\n",
    "```Python\n",
    "dictionary = corpora.Dictionary(doc_clean) #use gensium corpora to create data structure keeping all unique words\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] #use dictionary to create doc-term matrix for each of doc / file using bag of words approach\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3bad7-a05d-498d-8db5-6d9ad6f5da87",
   "metadata": {},
   "source": [
    "**Creating LDA model**: uses gensim lda models to set value of 5 for the first model to specify the number of topics for LDA\n",
    "```Python\n",
    "topic_num = 5\n",
    "word_num = 5\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = topic_num, id2word = dictionary, passes=20)\n",
    "\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))\n",
    "# Compute Perplexity\n",
    "print('Perplexity: ', ldamodel.log_perplexity(doc_term_matrix))\n",
    "```\n",
    "Some pointers: \n",
    "- the results and topics are often difficult to identify a category and may not be meaningful\n",
    "- how do you determine suitable number to use?\n",
    "  - use perplexity value (statistical measure of how well probability model predicts sample. benefit comes when comparing different LDA models and model with lower perplexity value is considered better\n",
    "- **increasing topic_num** to a large # MAY NOT HELP in understanding categories (unless prior knoweledge of possible large value), thus sacrificing clarity\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06f8b6-8e60-4da9-9646-8b540d6e92dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
